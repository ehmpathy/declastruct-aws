emit your response to the feedback into .behaviors/v2025_11_27.vpc-tunnel/3.3.blueprint.v2.i1.[feedback].v1.[taken].by_robot.md

emit your updated blueprint .behaviors/v2025_11_27.vpc-tunnel/3.3.blueprint.v3.i1.md

---

reboot your mechanics briefs first.

npx rhachet roles boot --repo ehmpathy --role mechanic

then address the blockers and nitpicks below

---

# blocker.1

instead of


const instance = await getEc2Instance({ by: { unique: input.by.instance } }, context);
      if (!instance?.instanceId)
        throw new UnexpectedCodePathError('ec2 instance not found', { input });

=>


      const instance = await getEc2Instance({ by: { unique: input.by.instance } }, context) ??
  BadRequestEror.throw('cant find instance to set status of. does it exist?', { input })


that way its clear its a bad request error; also, no need for extra if


also, getEc2Instance should guarantee that instanceId is returned in output

just rename the key from `instanceId` => `id`, since instance is already implied by the domain entity name && that way we can use default `HasMetadata` on the return type of getEc2Instance

---


# blocker.2


export const setEc2InstanceStatus = asProcedure(
  async (
    input: {
      by: {
        instance: RefByUnique<typeof DeclaredAwsEc2Instance>;
      };


=>


export const setEc2InstanceStatus = asProcedure(
  async (
    input: {
      by: {
        instance: Ref<typeof DeclaredAwsEc2Instance>;
      };

support both refs, by unique and by primary


---

# blocker.3


no if/else

use an IFFE with early returns and failfast if neither early return was satisfied


    // start or stop based on desired status
    if (input.to.status === 'running') {
      await ec2.send(new StartInstancesCommand({ InstanceIds: [instance.instanceId] }));
      await waitUntilInstanceRunning(
        { client: ec2, maxWaitTime: 300 },
        { InstanceIds: [instance.instanceId] },
      );
    } else {
      await ec2.send(new StopInstancesCommand({ InstanceIds: [instance.instanceId] }));
      await waitUntilInstanceStopped(
        { client: ec2, maxWaitTime: 300 },
        { InstanceIds: [instance.instanceId] },
      );
    }


===

# nitpick.4


    // return updated instance
    const updated = await getEc2Instance({ by: { unique: input.by.instance } }, context);
    if (!updated) throw new UnexpectedCodePathError('instance disappeared after status change', { input });
    return updated;

    =>


    // return updated instance
    const updated = await getEc2Instance({ by: { unique: input.by.instance } }, context) ?? UnexpectedCodePathError.throw('how can instance not be found after status update?', { input, instance: { before: instance } });
    return updated;


====

# blocker.5


    server.once('error', (err: NodeJS.ErrnoException) => {
      resolve(err.code === 'EADDRINUSE');
    });

reject if the error is unexpected to failfast

otherwise, this is a potential failhide hazard


====

# blocker.6


    // ensure tunnels directory exists
    await fs.mkdir(TUNNELS_DIR, { recursive: true });


tunnels dir should come from ContextAws as an option w/ default set

e.g., under

`cache: { DeclaredAwsVpcTunnel: { processes: { dir: string } } }`


====

# blocker.7

getTunnelHash({
      instanceId: bastion.instanceId,
      remoteHost: cluster.host.writer,
      remotePort: cluster.port,
      localPort: input.from.port,
    });


    getTunnelHash should have input of


getTunnelHash({ for: { tunnel: RefByUnique<typeof DeclaredAwsVpcTunnel> }})

as input;

that way we talk in terms of our domain-objects always, rather than bags-of-words, to maximize our ubiquitous language


====

# blocker.8

no failhide opportunities allowed


      } catch {
        // pidfile doesn't exist or can't be read; tunnel already closed
      }

handle an allowlist of errors, if needed; failfast on any error not explicitly allowlisted


====

# blocker.9

why is this trycatching?


    if (portInUse) {
      // check if it's our tunnel
      try {
        const pidStr = await fs.readFile(pidPath, 'utf-8');
        const pid = parseInt(pidStr, 10);
        if (isProcessAlive({ pid })) {
          // verify tunnel is healthy
          const healthy = await isTunnelHealthy({ port: input.from.port });
          if (healthy) {
            return new DeclaredAwsVpcTunnel({ ...input, status: 'OPEN', pid }) as HasMetadata<DeclaredAwsVpcTunnel>;
          }
          // tunnel process alive but not healthy; kill and respawn
          process.kill(pid, 'SIGTERM');
        }
      } catch {
        // pidfile doesn't exist; another process owns the port
        throw new UnexpectedCodePathError('port is in use by another process', {
          port: input.from.port,
        });
      }
    }

avoid unnesseary try catches;

check for file existance explicitly, not via implicit error

create more of a narrative flow

and eliminate all failhide hazards in favor of failfast


===========


# blocker.10


    // persist tunnel state
    await fs.writeFile(pidPath, String(tunnelProcess.pid));
    await fs.writeFile(metaPath, JSON.stringify(input, null, 2));


persist tunnel state sooner so that if the command is exited earlier while the tunnel still instantiates in background ,we can try and wait for the tunnel still, rather than subsequently failing irrecoverably that there's a process using the port that's not ours
